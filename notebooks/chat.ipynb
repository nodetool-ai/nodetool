{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/conda/envs/genflow/lib/python3.11/site-packages/pydantic/_internal/_fields.py:200: UserWarning: Field name \"json\" in \"Body_create_api_assets__post\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n",
      "/usr/local/Cellar/conda/envs/genflow/lib/python3.11/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/usr/local/Cellar/conda/envs/genflow/lib/python3.11/site-packages/pydantic/_internal/_fields.py:200: UserWarning: Field name \"json\" in \"Body_create_api_assets__post\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'max_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     Message(role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that creates records about people.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     22\u001b[0m     Message(role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate a record for John Doe, who is 30 years old and lives in New York.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Process messages\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m assistant_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_messages(\n\u001b[1;32m     27\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m     28\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     30\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread_id,\n\u001b[1;32m     31\u001b[0m     node_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode123\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m assistant_message\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/chat/chat.py:547\u001b[0m, in \u001b[0;36mprocess_messages\u001b[0;34m(context, messages, model, thread_id, node_id, tools, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_messages\u001b[39m(\n\u001b[1;32m    520\u001b[0m     context: ProcessingContext,\n\u001b[1;32m    521\u001b[0m     messages: Sequence[Message],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message:\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    Process a message by the given model.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    Creates a completion using the provided messages.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m        tuple[Message, list[ToolCall]]: The assistant message and the tool calls.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m create_completion(\n\u001b[1;32m    548\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    549\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    550\u001b[0m         thread_id\u001b[38;5;241m=\u001b[39mthread_id,\n\u001b[1;32m    551\u001b[0m         node_id\u001b[38;5;241m=\u001b[39mnode_id,\n\u001b[1;32m    552\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[message_param(message) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages],\n\u001b[1;32m    553\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    555\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/chat/chat.py:311\u001b[0m, in \u001b[0;36mcreate_completion\u001b[0;34m(context, model, node_id, thread_id, messages, tools, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_completion\u001b[39m(\n\u001b[1;32m    302\u001b[0m     context: ProcessingContext,\n\u001b[1;32m    303\u001b[0m     model: FunctionModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    309\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m Provider\u001b[38;5;241m.\u001b[39mOpenAI:\n\u001b[0;32m--> 311\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_openai_completion(\n\u001b[1;32m    312\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    314\u001b[0m             node_id\u001b[38;5;241m=\u001b[39mnode_id,\n\u001b[1;32m    315\u001b[0m             messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    316\u001b[0m             tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    317\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    318\u001b[0m         )\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m Provider\u001b[38;5;241m.\u001b[39mAnthropic:\n\u001b[1;32m    320\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_anthropic_completion(\n\u001b[1;32m    321\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    322\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    327\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/chat/chat.py:180\u001b[0m, in \u001b[0;36mcreate_openai_completion\u001b[0;34m(context, model, node_id, messages, tools, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [tool\u001b[38;5;241m.\u001b[39mtool_param() \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[1;32m    178\u001b[0m openai_messages \u001b[38;5;241m=\u001b[39m [convert_to_openai_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[0;32m--> 180\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun_prediction(\n\u001b[1;32m    181\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    182\u001b[0m     provider\u001b[38;5;241m=\u001b[39mProvider\u001b[38;5;241m.\u001b[39mOpenAI,\n\u001b[1;32m    183\u001b[0m     node_id\u001b[38;5;241m=\u001b[39mnode_id,\n\u001b[1;32m    184\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: openai_messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m completion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo completion content returned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo completion content returned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/workflows/processing_context.py:333\u001b[0m, in \u001b[0;36mProcessingContext.run_prediction\u001b[0;34m(self, node_id, provider, model, params, data)\u001b[0m\n\u001b[1;32m    320\u001b[0m data_encoded \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    321\u001b[0m     base64\u001b[38;5;241m.\u001b[39mb64encode(data)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    324\u001b[0m req \u001b[38;5;241m=\u001b[39m PredictionCreateRequest(\n\u001b[1;32m    325\u001b[0m     provider\u001b[38;5;241m=\u001b[39mprovider,\n\u001b[1;32m    326\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata_encoded,\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi/predictions/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    336\u001b[0m     json\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mmodel_dump(),\n\u001b[1;32m    337\u001b[0m ):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cancelled:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobCancelledException()\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/common/nodetool_api_client.py:140\u001b[0m, in \u001b[0;36mNodetoolAPIClient.stream\u001b[0;34m(self, method, path, json, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m json \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson is required\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     req \u001b[38;5;241m=\u001b[39m PredictionCreateRequest(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjson)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m run_prediction(req, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_id):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m msg\u001b[38;5;241m.\u001b[39mmodel_dump_json() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/api/prediction.py:63\u001b[0m, in \u001b[0;36mrun_prediction\u001b[0;34m(req, user_id)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m msg\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m req\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m Provider\u001b[38;5;241m.\u001b[39mOpenAI:\n\u001b[0;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_openai(\n\u001b[1;32m     64\u001b[0m         prediction\u001b[38;5;241m=\u001b[39mprediction,\n\u001b[1;32m     65\u001b[0m         params\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mparams,\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m PredictionResult\u001b[38;5;241m.\u001b[39mfrom_result(Prediction\u001b[38;5;241m.\u001b[39mfrom_model(prediction), result)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m req\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m Provider\u001b[38;5;241m.\u001b[39mAnthropic:\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/providers/openai/prediction.py:118\u001b[0m, in \u001b[0;36mrun_openai\u001b[0;34m(prediction, params)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m create_embedding(prediction, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m create_chat_completion(prediction, params)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts-\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m create_speech(prediction, params)\n",
      "File \u001b[0;32m/usr/local/Cellar/nodetool/src/nodetool/providers/openai/prediction.py:51\u001b[0m, in \u001b[0;36mcreate_chat_completion\u001b[0;34m(prediction, params)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_chat_completion\u001b[39m(\n\u001b[1;32m     43\u001b[0m     prediction: Prediction,\n\u001b[1;32m     44\u001b[0m     params: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     46\u001b[0m     client \u001b[38;5;241m=\u001b[39m Environment\u001b[38;5;241m.\u001b[39mget_openai_client()\n\u001b[1;32m     48\u001b[0m     res: ChatCompletion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     49\u001b[0m         model\u001b[38;5;241m=\u001b[39mprediction\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     50\u001b[0m         messages\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 51\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     52\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     53\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     54\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     55\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     56\u001b[0m         response_format\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m res\u001b[38;5;241m.\u001b[39musage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     prediction\u001b[38;5;241m.\u001b[39mcost \u001b[38;5;241m=\u001b[39m calculate_cost_for_completion_usage(prediction\u001b[38;5;241m.\u001b[39mmodel, res\u001b[38;5;241m.\u001b[39musage)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'max_tokens'"
     ]
    }
   ],
   "source": [
    "from nodetool.nodes.nodetool.agents import CreateRecordTool\n",
    "from nodetool.workflows.processing_context import ProcessingContext\n",
    "from nodetool.metadata.types import ColumnDef, Message, FunctionModel, Provider\n",
    "from nodetool.chat.chat import process_messages, process_tool_calls\n",
    "\n",
    "\n",
    "context = ProcessingContext(user_id=\"user123\", auth_token=\"token123\")\n",
    "thread_id = \"thread456\"\n",
    "model = FunctionModel(name=\"gpt-4o\", provider=Provider.OpenAI)\n",
    "\n",
    "columns = [\n",
    "    ColumnDef(name=\"name\", data_type=\"string\", description=\"Person's name\"),\n",
    "    ColumnDef(name=\"age\", data_type=\"int\", description=\"Person's age\"),\n",
    "    ColumnDef(name=\"city\", data_type=\"string\", description=\"City of residence\")\n",
    "]\n",
    "\n",
    "tools = [CreateRecordTool(columns)]\n",
    "\n",
    "# Example messages\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"You are a helpful assistant that creates records about people.\"),\n",
    "    Message(role=\"user\", content=\"Create a record for John Doe, who is 30 years old and lives in New York.\")\n",
    "]\n",
    "\n",
    "# Process messages\n",
    "assistant_message = await process_messages(\n",
    "    context=context,\n",
    "    messages=messages,\n",
    "    model=model,\n",
    "    thread_id=thread_id,\n",
    "    node_id=\"node123\",\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* [[TOOL]] create_record *******\n",
      "******* [[TOOL RESPONSE]] {'name': 'John Doe', 'age': 30, 'city': 'New York'} *******\n"
     ]
    }
   ],
   "source": [
    "response = await process_tool_calls(\n",
    "    context=context,\n",
    "    thread_id=thread_id,\n",
    "    message=assistant_message,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(type='message', id='6c9aebde308f11efae0800002c60059c', thread_id='thread456', user_id=None, tool_call_id=None, role='assistant', name='', content='A record has been created for John Doe, who is 30 years old and lives in New York.', tool_calls=[], created_at='2024-06-22T14:03:23.232327')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
